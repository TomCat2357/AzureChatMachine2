# %%

import streamlit as st
import logging
from logging.handlers import TimedRotatingFileHandler


def initialize_logger(user_id=""):
    class CustomLogger(logging.LoggerAdapter):
        def __init__(self, logger, user_id):
            super().__init__(logger, {})
            self.user_id = user_id

        def process(self, msg, kwargs):
            return f"{self.user_id} - {msg}", kwargs
    #ãƒ­ã‚¬ãƒ¼ã‚’åˆæœŸåŒ–ã™ã‚‹é–¢æ•°
    # ãƒ­ã‚¬ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ã¾ãŸã¯ä½œæˆã—ã¾ã™
    logger = logging.getLogger(__name__)
    # ãƒ­ã‚¬ãƒ¼ã®ãƒ¬ãƒ™ãƒ«ã‚’DEBUGã«è¨­å®šã—ã¾ã™
    logger.setLevel(logging.DEBUG)

    # ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’è¨­å®šã—ã¾ã™
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã¸ã®ãƒãƒ³ãƒ‰ãƒ©ã‚’ä½œæˆã—ã€è¨­å®šã—ã¾ã™
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.DEBUG)
    console_handler.setFormatter(formatter)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒãƒ³ãƒ‰ãƒ©ã‚’ä½œæˆã—ã€è¨­å®šã—ã¾ã™
    file_handler = TimedRotatingFileHandler(
        "../log/streamlit_logfile.log", when="midnight", interval=1, backupCount=7
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥ä»˜å½¢å¼ã‚’è¨­å®šã—ã¾ã™
    file_handler.suffix = "%Y-%m-%d"

    # ãƒ­ã‚¬ãƒ¼ã«ãƒãƒ³ãƒ‰ãƒ©ã‚’è¿½åŠ ã—ã¾ã™
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    if user_id:
        return CustomLogger(logger, user_id)
    else:
        return logger

USER_ID = "TESTID"

# Streamlitã®session_stateã‚’ä½¿ã£ã¦ãƒ­ã‚¬ãƒ¼ãŒåˆæœŸåŒ–ã•ã‚ŒãŸã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
if "logger_initialized" not in st.session_state:
    logger = initialize_logger(USER_ID)
    st.session_state["logger_initialized"] = True
else:
    logger = logging.getLogger(__name__)

from typing import Any, List, Generator, Iterable
import openai, os, redis, time, json, tiktoken, datetime, requests
import numpy as np
from threading import Thread
from queue import Queue, Empty
from copy import copy
from concurrent.futures import ThreadPoolExecutor

executor1 = ThreadPoolExecutor(1)

redisCliPrompt = redis.Redis(host="redis_6379", port=6379, db=0)
redisCliUserSetting = redis.Redis(host="redis_6379", port=6379, db=1)
redisCliPastChat = redis.Redis(host="redis_6379", port=6379, db=2)
redisCliAccessTime = redis.Redis(host="redis_6379", port=6379, db=3)


def trim_tokens(
    messages: List[dict], max_tokens: int, model_name: str = "gpt-3.5-turbo-0301"
) -> List[dict]:
    """
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒæŒ‡å®šã—ãŸæœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¶…ãˆã‚‹å ´åˆã€
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å…ˆé ­ã‹ã‚‰é †ã«å‰Šé™¤ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ä»¥ä¸‹ã«ä¿ã¤ã€‚

    å¼•æ•°:
        messages (List[dict]): ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã€‚
        max_tokens (int): æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã€‚
        model_name (str): ãƒ¢ãƒ‡ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯'gpt-3.5-turbo-0301'ï¼‰ã€‚

    æˆ»ã‚Šå€¤:
        List[dict]: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒæœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ä»¥ä¸‹ã«ãªã£ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã€‚
    """
    # ç„¡é™ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹
    while True:
        # ç¾åœ¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
        total_tokens = calc_token_tiktoken(str(messages), model_name=model_name)
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒæœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ä»¥ä¸‹ã«ãªã£ãŸå ´åˆã€ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†
        if total_tokens <= max_tokens:
            break
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒæœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¶…ãˆã¦ã„ã‚‹å ´åˆã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å…ˆé ­ã‚’å‰Šé™¤
        messages.pop(0)

    # ä¿®æ­£ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¿½ï¿½ï¿½ãƒªã‚¹ãƒˆã‚’è¿”ã™
    return messages


def response_chatgpt(
    prompt: List[dict], model_name: str = "gpt-3.5-turbo", stream: bool = True
) -> Generator:
    """
    ChatGPTã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ã—ã¾ã™ã€‚

    å¼•æ•°:
        prompt (List[dict]): éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå…¥ã£ãŸãƒªã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‚
        model_name (str): ä½¿ç”¨ã™ã‚‹ChatGPTã®ãƒ¢ãƒ‡ãƒ«åã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯"gpt-3.5-turbo"ã€‚

    æˆ»ã‚Šå€¤:
        response: ChatGPTã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€‚
    """
    # logger.debug(role(user_msg))
    logger.debug(f"trim_tokenså‰ã®prompt: {prompt}")
    logger.debug(f"trim_tokenså‰ã®promptã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {calc_token_tiktoken(str(prompt))}")
    # logger.debug(f"trim_tokenså‰ã®messages_role: {type(messages)}")

    prompt = trim_tokens(prompt, INPUT_MAX_TOKENS)
    logger.debug(f"trim_tokenså¾Œã®prompt: {str(prompt)}")
    logger.debug(f"trim_tokenså¾Œã®promptã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {calc_token_tiktoken(str(prompt))}")
    try:
        logger.info(
            f"Sending request to OpenAI API with prompt: {prompt}, model_name : {model_name}"
        )

        response = openai.ChatCompletion.create(
            model=model_name,
            messages=prompt,
            stream=stream,
        )
    except Exception as e:
        logger.error(f"Error while communicating with OpenAI API: {e}")
        raise

    return response


def calc_token_tiktoken(
    chat: str, encoding_name: str = "", model_name: str = "gpt-3.5-turbo-0301"
) -> int:
    """
    # å¼•æ•°ã®èª¬æ˜:
    # chat: ãƒˆãƒ¼ã‚¯ï¿½ï¿½æ•°ã‚’è¨ˆç®—ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã€‚ã“ã®ãƒ†ã‚­ã‚¹ãƒˆãŒAIãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ã©ã®ã‚ˆã†ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã‹ã‚’åˆ†æã—ã¾ã™ã€‚

    # encoding_name: ä½¿ç”¨ã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®åå‰ã€‚ã“ã®å¼•æ•°ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ãã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
    # ä¾‹ãˆã° 'utf-8' ã‚„ 'ascii' ãªã©ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åã‚’æŒ‡å®šã§ãã¾ã™ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ã€model_nameã«åŸºã¥ã„ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒé¸ã°ã‚Œã¾ã™ã€‚

    # model_name: ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã®åå‰ã€‚ã“ã®å¼•æ•°ã¯ã€ç‰¹å®šã®AIãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•ã§é¸æŠã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
    # ä¾‹ãˆã° 'gpt-3.5-turbo-0301' ã¨ã„ã†ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®šã™ã‚Œã°ã€ãã®ãƒ¢ãƒ‡ãƒ«ã«é©ã—ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒé¸ã°ã‚Œã¾ã™ã€‚
    # encoding_nameãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿ã€ã“ã®å¼•æ•°ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
    """
    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ±ºå®šã™ã‚‹
    if encoding_name:
        # encoding_nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ã€ãã®åå‰ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—ã™ã‚‹
        encoding = tiktoken.get_encoding(encoding_name)
    elif model_name:
        # model_nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ã€ãã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—ã™ã‚‹
        encoding = tiktoken.get_encoding(tiktoken.encoding_for_model(model_name).name)
    else:
        # ä¸¡æ–¹ã¨ã‚‚æŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’æŠ•ã’ã‚‹
        raise ValueError("Both encoding_name and model_name are missing.")

    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ã—ã€ãã®æ•°ã‚’æ•°ãˆã‚‹
    num_tokens = len(encoding.encode(chat))
    return num_tokens


def check_rate_limit_exceed(
    redis_client: redis.Redis,
    key_name: str = "access",
    late_limit: int = 1,
    late_limit_period: float = 1.0,
) -> bool:
    """
    Checks if the rate limit exceeds for a given period.

    Args:
        redis_client (redis.Redis): The Redis client object.
        key_name : Redis client key name.
        late_limit (int, optional): The maximum number of access data within a certain period. Defaults to 1.
        late_limit_period (float, optional): The period in seconds. If the number of access data within this period is less than `late_limit`, the data from the previous day is also retrieved. Defaults to 1.0.

    Returns:
        bool: True if the rate limit is exceeded, False otherwise.
    """
    # Get the current timestamp
    now = time.time()
    # Retrieve the access data for the current date from the Redis client
    access_data = redis_client.zrangebyscore(
        key_name,
        now - late_limit_period,
        "+inf",
    )

    # Log the number of past access data
    logger.debug(f"Number of past access data: {len(access_data)}")
    # If the number of access data is less than the late limit, return False
    if len(access_data) < late_limit:
        return False
    # Otherwise, return True
    else:
        return True




# APIã‚­ãƒ¼ã®è¨­å®š
openai.api_key = os.environ["OPENAI_API_KEY"]
ASSISTANT_WARNING = (
    "æ³¨æ„ï¼šç§ã¯AIãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã€æƒ…å ±ãŒå¸¸ã«æœ€æ–°ã¾ãŸã¯æ­£ç¢ºã§ã‚ã‚‹ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚é‡è¦ãªæ±ºå®šã‚’ã™ã‚‹å‰ã«ã¯ã€ä»–ã®ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
)
# åˆ©ç”¨å¯èƒ½ãªGPTãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
AVAILABLE_MODELS : dict = json.loads(os.environ["AVAILABLE_MODELS"])

LATE_LIMIT : dict = json.loads(os.environ["LATE_LIMIT"])
LATE_LIMIT_COUNT : int = LATE_LIMIT["COUNT"]
LATE_LIMIT_PERIOD : float = LATE_LIMIT["PERIOD"]


# %%
# Streamlitã‚¢ãƒ—ãƒªã®é–‹å§‹æ™‚ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–
if "id" not in st.session_state:
    logger.debug("session initialized")
    st.session_state["id"] = "{}_{:0>20}".format(USER_ID, int(time.time_ns()))
    # ã‚‚ã—USER_IDã«å¯¾å¿œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€æœ€åˆã®åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
    if not redisCliUserSetting.hexists(USER_ID, "model"):
        redisCliUserSetting.hset(USER_ID, "model", list(AVAILABLE_MODELS.keys())[0])
    # ã‚‚ã—USER_IDã«å¯¾å¿œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ãªã„å ´åˆã€æœ€åˆã®åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
    elif redisCliUserSetting.hget(USER_ID, "model").decode() not in AVAILABLE_MODELS:
        redisCliUserSetting.hset(USER_ID, "model", list(AVAILABLE_MODELS.keys())[0])


# éå»ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
# INPUT_MAX_TOKENS = 20

logger.debug(f"session_id first : {st.session_state.id}")

st.title("Streamlitã®ChatGPTã‚µãƒ³ãƒ—ãƒ«")

# å®šæ•°å®šç¾©


# Streamlitã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«åˆ©ç”¨å¯èƒ½ãªGPTãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ãŸã‚ã®ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’è¿½åŠ 
redisCliUserSetting.hset(
    USER_ID,
    "model",
    st.sidebar.selectbox(
        "GPTãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",  # GPTãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ãŸã‚ã®ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’è¡¨ç¤º
        AVAILABLE_MODELS,  # åˆ©ç”¨å¯èƒ½ãªGPTãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
        index=list(AVAILABLE_MODELS).index(  # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            redisCliUserSetting.hget(USER_ID, "model").decode()  # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        ),
    ),  # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
)
INPUT_MAX_TOKENS = AVAILABLE_MODELS[redisCliUserSetting.hget(USER_ID, "model").decode()]


# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ã€ŒNew chatã€ãƒœã‚¿ãƒ³ã‚’è¿½åŠ ã—ã¾ã™ã€‚
# ãƒœã‚¿ãƒ³ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸã¨ãã«ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å†å®Ÿè¡Œã—ã¾ã™ã€‚
if st.sidebar.button("ğŸ”„ **New chat**"):
    del st.session_state["id"]
    st.rerun()

# 7æ—¥åˆ†ã®æ™‚é–“ã‚’æˆ»ã‚‹
seven_days_ago = datetime.datetime.now()
seven_days_ago_midnight = seven_days_ago.replace(
    hour=0, minute=0, second=0, microsecond=0
)
seven_days_ago_unixtime = int(seven_days_ago_midnight.timestamp())
session_id_with_chat_num_within_last_seven_days = redisCliAccessTime.zrangebyscore(
    "access", seven_days_ago_unixtime, "+inf"
)
session_id_within_last_seven_days = {
    "_".join(id_num.decode().split("_")[:-1])
    for id_num in session_id_with_chat_num_within_last_seven_days
}
user_chats = redisCliPastChat.hgetall(USER_ID)
user_chats_within_last_seven_days: dict = {
    session_id.decode(): json.loads(chat_data)
    for session_id, chat_data in user_chats.items()
    if session_id.decode() in session_id_within_last_seven_days
}

user_chats_within_last_seven_days_sorted: list[tuple] = sorted(
    user_chats_within_last_seven_days.items(), reverse=True
)

st.sidebar.markdown(
    "<p style='font-size:20px; color:#FFFF00;'>éå»ã®ãƒãƒ£ãƒƒãƒˆ</p>", unsafe_allow_html=True
)

for session_id, info in user_chats_within_last_seven_days_sorted:
    # print(info)
    title = info["title"]
    if len(title) > 15:
        title = title[:15] + "..."
    if st.sidebar.button(title):
        # ãƒœã‚¿ãƒ³ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸå ´åˆã€session_idã‚’st.session_state.idã«ä»£å…¥
        st.session_state["id"] = session_id
        # logger.debug(f'sessin id button : {st.session_state.id} clicked')
        # ç”»é¢ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥
        st.rerun()

# ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‹ã‚‰ã®è­¦å‘Šã‚’è¼‰ã›ã‚‹
with st.chat_message("assistant"):
    st.write(ASSISTANT_WARNING)


# ä»¥å‰ã®ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’è¡¨ç¤º
for chat in redisCliPrompt.lrange(st.session_state.id, 0, -1):
    chat = json.loads(chat)
    with st.chat_message(chat["role"]):
        st.write(chat["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
user_msg: str = st.chat_input("ã“ã“ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›")

# å‡¦ç†é–‹å§‹
if user_msg:
    # logger.debug(f'session_id second : {st.session_state.id}')

    # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    with st.chat_message("user"):
        st.write(user_msg)
    new_prompt: dict = {"role": "user", "content": user_msg}
    redisCliPrompt.rpush(st.session_state.id, json.dumps(new_prompt))
    error_flag = False
    try:
        # å…¥åŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
        user_msg_tokens: int = calc_token_tiktoken(str([new_prompt]))
        logger.debug(f"å…¥åŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {user_msg_tokens}")
        if user_msg_tokens > INPUT_MAX_TOKENS:
            # st.text_area("å…¥åŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸", user_msg, height=100)  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å†è¡¨ç¤º
            # st.warning("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒé•·ã™ãã¾ã™ã€‚çŸ­ãã—ã¦ãã ã•ã„ã€‚" f"({user_msg_tokens}tokens)")
            raise Exception("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒé•·ã™ãã¾ã™ã€‚çŸ­ãã—ã¦ãã ã•ã„ã€‚" f"({user_msg_tokens}tokens)")
        if check_rate_limit_exceed(
            redisCliAccessTime,
            key_name="access",
            late_limit=LATE_LIMIT_COUNT,
            late_limit_period=LATE_LIMIT_PERIOD,
        ):
            raise Exception("ã‚¢ã‚¯ã‚»ã‚¹æ•°ãŒå¤šã„ãŸã‚ã€æ¥ç¶šã§ãã¾ã›ã‚“ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚")
        prompt = [
            json.loads(prompt)
            for prompt in redisCliPrompt.lrange(st.session_state.id, 0, -1)
        ]
        response = response_chatgpt(
            prompt,
            model_name=redisCliUserSetting.hget(USER_ID, "model").decode(),
            stream=True,
        )
    except Exception as e:
        error_flag = True
        st.warning(e)
        # ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã®ã§ä»Šå›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤ã™ã‚‹
        redisCliPrompt.rpop(st.session_state.id, 1)
    if not error_flag:
        now = time.time()
        redisCliAccessTime.zadd(
            "access",
            {
                f"{st.session_state.id}_{redisCliPrompt.llen(st.session_state.id):0>6}": now
            },
        )

        def redisCliPastChatRecord(prompt, timestamp, model, session_id):
            # logger.debug('redisCliPastChatRecord start')
            if not redisCliPastChat.hexists(USER_ID, session_id):
                prompt_for_title = copy(prompt[0])
                add_prompt = "ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰é©åˆ‡ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ä¸»è¦ãªå†…å®¹ã¨ãƒˆãƒ¼ãƒ³ã‚’è€ƒæ…®ã—ã€ç°¡æ½”ã‹ã¤çš„ç¢ºãªã‚¿ã‚¤ãƒˆãƒ«ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: "
                add_prompt_token_num = calc_token_tiktoken(add_prompt)
                count = 0
                while True:
                    if (
                        INPUT_MAX_TOKENS
                        >= calc_token_tiktoken(str([prompt_for_title]))
                        + add_prompt_token_num
                    ):
                        break
                    count += 1
                    if count > 100:
                        return
                    prompt_for_title["content"] = prompt_for_title["content"][:-1]
                prompt_for_title["content"] = add_prompt + prompt_for_title["content"]
                # logger.debug(f'prompt_for_title : {prompt_for_title}')
                response = response_chatgpt(
                    [prompt_for_title],
                    model_name="gpt-3.5-turbo",
                    stream=False,
                )
                title = response["choices"][0]["message"].get("content", "")
                logger.info(f"Response for title : {title}")
                # logger.debug(f"title : {title}")
            else:
                title = json.loads(redisCliPastChat.hget(USER_ID, session_id))["title"]
            redisCliPastChat.hset(
                USER_ID,
                session_id,
                json.dumps({"timestamp": timestamp, "model": model, "title": title}),
            )
            # except Exception:
            #    traceback.print_exc()

        # logger.debug('redisCliPastChatRecord submit')

        executor1.submit(
            redisCliPastChatRecord,
            prompt,
            now,
            redisCliUserSetting.hget(USER_ID, "model").decode(),
            st.session_state.id,
        )

        assistant_prompt = {"role": "assistant", "content": ""}
        redisCliPrompt.rpush(st.session_state.id, json.dumps(assistant_prompt))
        prompt_length = redisCliPrompt.llen(st.session_state.id)

        with st.chat_message("assistant"):
            assistant_msg = ""
            assistant_response_area = st.empty()
            for chunk in response:
                # å›ç­”ã‚’é€æ¬¡è¡¨ç¤º
                tmp_assistant_msg = chunk["choices"][0]["delta"].get("content", "")
                assistant_msg += tmp_assistant_msg
                assistant_prompt["content"] = assistant_msg
                redisCliPrompt.lset(
                    st.session_state.id, prompt_length - 1, json.dumps(assistant_prompt)
                )
                assistant_response_area.write(assistant_msg)
            logger.info(f"Response for chat : {assistant_msg}")
        # logger.debug('Rerun')
        st.rerun()

    # å‡¦ç†çµ‚äº†

